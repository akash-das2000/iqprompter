def evaluate(prompt: str, response: str) -> dict:
    """
    Compute all metric scores for a prompt/response pair.
    Returns a dict, e.g. {"hallucination": 0.0, "relevance": 1.0}.
    """
    return {}
